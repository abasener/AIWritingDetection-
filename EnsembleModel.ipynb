{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to /home/abby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/abby/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading graphviz: Package 'graphviz' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk\n",
    "# pip install lazypredict\n",
    "# pip install seaborn\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('graphviz')\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import sys\n",
    "from nltk import ngrams\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from nltk import FreqDist\n",
    "import statistics\n",
    "import numpy as np \n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import plot_tree\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.optimize import approx_fprime\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There are overlapping file IDs between original and generated data.\n",
      "2001\n",
      "Length of Human Data: 669386\n",
      "Length of Generated Data: 404577\n",
      "All data size:  1073963\n",
      "=====\n",
      "Using Words\n"
     ]
    }
   ],
   "source": [
    "RunName = 'AllTest'\n",
    "# 1 - Words, 2 - Type, 3 - BiGram Words, 4 - BiGram Type\n",
    "case_Type = 1\n",
    "# 1 - Boolean, 2 - Counts, 3 - Just feature set\n",
    "case_BoolTF = 3\n",
    "\n",
    "# Feachure set for report\n",
    "control = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]#[1,1,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the paths to the two directories\n",
    "original_dir = r\"./DataSet2/benchmark_set/human-written\"\n",
    "generated_dir = r\"./DataSet2/benchmark_set/ai-generated\"\n",
    "\n",
    "# Get the files for the original\n",
    "original_file_ids = [file_id for file_id in os.listdir(original_dir) if file_id.endswith(\".txt\")]\n",
    "file_original = []\n",
    "for file_id in original_file_ids:\n",
    "    file_path = os.path.join(original_dir, file_id)\n",
    "    #print(f\"Reading original file: {file_path}\")  # Print file path for debugging\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        file_original.append((file.read(), \"org\"))  # Tuple with text and class \"org\"\n",
    "\n",
    "# Get the files for the generated\n",
    "generated_file_ids = [file_id for file_id in os.listdir(generated_dir) if file_id.endswith(\".txt\")]\n",
    "file_generated = []\n",
    "for file_id in generated_file_ids:\n",
    "    file_path = os.path.join(generated_dir, file_id)\n",
    "    #print(f\"Reading generated file: {file_path}\")  # Print file path for debugging\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        file_generated.append((file.read(), \"gen\"))  # Tuple with text and class \"gen\"\n",
    "\n",
    "# Check if there are overlapping file IDs between the two directories\n",
    "if set(original_file_ids).intersection(set(generated_file_ids)):\n",
    "    print(\"Warning: There are overlapping file IDs between original and generated data.\")\n",
    "else:\n",
    "    print(\"No overlapping file IDs between original and generated data.\")\n",
    "\n",
    "# Merge the files into one string\n",
    "merged_original = '{*%*}'.join([text for text, _ in file_original])\n",
    "merged_generated = '{*%*}'.join([text for text, _ in file_generated])\n",
    "\n",
    "# Combine the data from both classes\n",
    "documents = file_original + file_generated\n",
    "print(len(documents))\n",
    "# Get as words\n",
    "all_words = []\n",
    "short_org_words = word_tokenize(merged_original)\n",
    "short_gen_words = word_tokenize(merged_generated)\n",
    "\n",
    "for w in short_org_words:\n",
    "    all_words.append(w.lower())\n",
    "for w in short_gen_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "print(\"Length of Human Data:\", len(short_org_words)) # Number of words\n",
    "print(\"Length of Generated Data:\", len(short_gen_words)) # Number of words\n",
    "print(\"All data size: \", len(all_words))    # Todal number of words\n",
    "print(\"=====\")\n",
    "def clean(text, punF, numF, stopF):\n",
    "    # remove numbers\n",
    "    if numF == 1:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    # remove punctuations\n",
    "    if punF == 1:\n",
    "        text = \"\".join([char for char in text if char not in string.punctuation]) \n",
    "    # remove stop words\n",
    "    if stopF == 1:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = [w for w in word_tokenize(text) if w.lower() not in stop_words]\n",
    "        text = ' '.join(text)  # Join the list of words into a single string\n",
    "    return text\n",
    "\n",
    "# output = [clean(word,1,1,1) for word in all_words]\n",
    "# print(\"Cleaned data size: \",len((all_words))) # Number of cleaned words\n",
    "\n",
    "#case_Type = 1\n",
    "\n",
    "if case_Type == 1:\n",
    "    # Set up functions\n",
    "    def find_features_count(document,word_features):\n",
    "        words = document.split()\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = words.count(w)\n",
    "        return features\n",
    "    \n",
    "    def find_features_boolean(document,word_features):\n",
    "        words = set(document)\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = (w in words)\n",
    "        return features\n",
    "\n",
    "    #Clean Words\n",
    "    cleaned_words = [clean(word,1,1,1) for word in all_words]\n",
    "    # Pulling feachures\n",
    "    if(TestingOutput == True):\n",
    "        print(\"All data size: \", len((all_words)))\n",
    "        print(\"Cleaned data size: \", len((cleaned_words)))\n",
    "    all_fetr = cleaned_words\n",
    "    print(\"Using Words\")\n",
    "    \n",
    "elif case_Type == 2:\n",
    "    # Set up functoins \n",
    "    def find_features_count(document,word_features):\n",
    "        foo = nltk.pos_tag(word_tokenize(document))\n",
    "        document = [pos_tag for (word, pos_tag) in foo]\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = document.count(w)\n",
    "        return features\n",
    "    \n",
    "    def find_features_boolean(document,word_features):\n",
    "        foo = nltk.pos_tag(word_tokenize(document))\n",
    "        document = [pos_tag for (word, pos_tag) in foo]\n",
    "        words = set(document)\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = (w in words)\n",
    "        return features\n",
    "    \n",
    "    #Clean Words\n",
    "    cleaned_words = [clean(word,1,1,1) for word in all_words]\n",
    "    # Pulling feachures\n",
    "    if(TestingOutput == True):\n",
    "        print(\"All data size: \", len((all_words)))\n",
    "        print(\"Cleaned data size: \", len((cleaned_words)))\n",
    "    all_fetr = nltk.pos_tag(cleaned_words)\n",
    "    all_fetr = [pos_tag for (word, pos_tag) in all_fetr]\n",
    "    print(\"Using Word's Type\")\n",
    "\n",
    "elif case_Type == 3:\n",
    "    # Function to generate n-grams\n",
    "    def find_features_count(document,word_features):\n",
    "        foo = word_tokenize(document[:])\n",
    "        document = list(bigrams(foo))\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = document.count(w)\n",
    "        return features\n",
    "    \n",
    "    def find_features_boolean(document,word_features):\n",
    "        foo = word_tokenize(document[:])\n",
    "        document = list(bigrams(foo))\n",
    "        words = set(document)\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = (w in words)\n",
    "        return features\n",
    "    \n",
    "    #Clean Words\n",
    "    cleaned_words = [clean(word,0,0,0) for word in all_words]\n",
    "    if(TestingOutput == True):\n",
    "        print(\"All data size: \", len((all_words)))\n",
    "        print(\"Cleaned data size: \", len((cleaned_words)))\n",
    "    all_fetr = list(bigrams(cleaned_words[:]))\n",
    "    print(\"Using BiGram for Word\")\n",
    "\n",
    "elif case_Type == 4:\n",
    "    # Function to generate n-grams\n",
    "    def find_features_count(document,word_features):\n",
    "        foo = nltk.pos_tag(word_tokenize(document))\n",
    "        document = [pos_tag for (word, pos_tag) in foo]\n",
    "        #foo = word_tokenize(document[:])\n",
    "        document = list(bigrams(document))\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = document.count(w)\n",
    "        return features\n",
    "    \n",
    "    def find_features_boolean(document,word_features):\n",
    "        foo = nltk.pos_tag(word_tokenize(document))\n",
    "        document = [pos_tag for (word, pos_tag) in foo]\n",
    "        #foo = word_tokenize(document[:])\n",
    "        document = list(bigrams(document))\n",
    "        words = set(document)\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = (w in words)\n",
    "        return features\n",
    "    \n",
    "    #Clean Words\n",
    "    cleaned_words = [clean(word,0,0,0) for word in all_words]\n",
    "    if(TestingOutput == True):\n",
    "        print(\"All data size: \", len((all_words)))\n",
    "        print(\"Cleaned data size: \", len((cleaned_words)))\n",
    "    all_fetr = nltk.pos_tag(cleaned_words)\n",
    "    all_fetr = [pos_tag for (word, pos_tag) in all_fetr]\n",
    "    all_fetr = list(bigrams(all_fetr[:]))\n",
    "    print(\"Using BiGram for Type\")\n",
    "else:\n",
    "    print(\"case_value out of bounds\")\n",
    "if(TestingOutput == True):\n",
    "    if (case_Type < 5):\n",
    "        print(all_fetr[:15])\n",
    "all_fetr = nltk.FreqDist(all_fetr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features_stats(document, control):\n",
    "        # Set Up\n",
    "        sentences = sent_tokenize(document)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        sentence_lengths = [len(tokenizer.tokenize(sentence)) for sentence in sentences]\n",
    "        #sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "        paragraphs = document.split('\\n\\n') \n",
    "        features = {}\n",
    "        #print(sentence_lengths)\n",
    "\n",
    "        # sentences per paragraph\n",
    "        if control[0] == 1:\n",
    "            sent_per_paragraph = [len(sent_tokenize(paragraph)) for paragraph in paragraphs]\n",
    "            mean_sent_per_paragraph = np.mean(sent_per_paragraph)\n",
    "            features['Sentences per paragraph'] = mean_sent_per_paragraph\n",
    "\n",
    "        # words per paragraph\n",
    "        if control[1] == 1:\n",
    "            words_per_paragraph = [len(word_tokenize(paragraph)) for paragraph in paragraphs]\n",
    "            mean_words_per_paragraph = np.mean(words_per_paragraph)\n",
    "            features['Words per paragraph'] = mean_words_per_paragraph\n",
    "\n",
    "        # Number of occurrences of ')'\n",
    "        if control[2] == 1:\n",
    "            var = document.count(')')\n",
    "            features['Number of occurrences of )'] = var\n",
    "\n",
    "        # Number of occurrences of '-'\n",
    "        if control[3] == 1:\n",
    "            var = document.count('-')\n",
    "            features['Number of occurrences of -'] = var\n",
    "\n",
    "        # Number of occurrences of ';'\n",
    "        if control[4] == 1:\n",
    "            var = document.count(';')\n",
    "            features['Number of occurrences of ;'] = var\n",
    "        \n",
    "        # Number of occurrences of '?'\n",
    "        if control[5] == 1:\n",
    "            var = document.count('?')\n",
    "            features['Number of occurrences of ?'] = var\n",
    "            \n",
    "        # Number of occurrences of '''\n",
    "        if control[6] == 1:\n",
    "            var = document.count('\\'')\n",
    "            features['Number of occurrences of \\''] = var\n",
    "\n",
    "        # Sentence STD\n",
    "        if control[7] == 1:\n",
    "            std_dev = np.std(sentence_lengths) # Sample STD\n",
    "            features['Sentence STD'] = std_dev\n",
    "\n",
    "        # Length difference for consecutive sentences\n",
    "        if control[8] == 1:\n",
    "            length_diff = [sentence_lengths[i] - sentence_lengths[i - 1] for i in range(1, len(sentence_lengths))]\n",
    "            # Add mean length difference as a feature if there is a valid denominator\n",
    "            if len(length_diff) > 0:\n",
    "                mean_length_diff = np.mean(length_diff)\n",
    "                features['Length difference for consecutive sentences'] = mean_length_diff\n",
    "            else:\n",
    "                # Handle the case when there is no valid denominator (e.g., only one sentence)\n",
    "                features['Length difference for consecutive sentences'] = 0.0  # Set a default value or choose an appropriate handling\n",
    "        # Sentence with <11 words\n",
    "        if control[9] == 1:\n",
    "            # Count sentences with <11 words\n",
    "            short_sentences = [sentence for sentence in sentence_lengths if sentence < 11]\n",
    "            num_short_sentences = len(short_sentences)\n",
    "            # Add the count as a feature\n",
    "            features['Sentence with <11 words'] = num_short_sentences\n",
    "\n",
    "        # Sentence with >34 words\n",
    "        if control[10] == 1:\n",
    "            # Count sentences with <11 words\n",
    "            short_sentences = [sentence for sentence in sentence_lengths if sentence > 34]\n",
    "            num_short_sentences = len(short_sentences)\n",
    "            # Add the count as a feature\n",
    "            features['Sentence with >34 words'] = num_short_sentences\n",
    "\n",
    "        # Number of occurrences of 'although'\n",
    "        if control[11] == 1:\n",
    "            var = document.count('although')\n",
    "            features['Number of occurrences of although'] = var\n",
    "\n",
    "        # Number of occurrences of 'However'\n",
    "        if control[12] == 1:\n",
    "            var = document.count('However')\n",
    "            features['Number of occurrences of However'] = var\n",
    "\n",
    "        # Number of occurrences of 'but'\n",
    "        if control[13] == 1:\n",
    "            var = document.count('but')\n",
    "            features['Number of occurrences of but'] = var\n",
    "\n",
    "        # Number of occurrences of 'because'\n",
    "        if control[14] == 1:\n",
    "            var = document.count('because')\n",
    "            features['Number of occurrences of because'] = var\n",
    "\n",
    "        # Number of occurrences of 'this'\n",
    "        if control[15] == 1:\n",
    "            var = document.count('this')\n",
    "            features['Number of occurrences of this'] = var\n",
    "\n",
    "        # Number of occurrences of ‘others’ or ‘researchers’\n",
    "        if control[16] == 1:\n",
    "            var = document.lower().count('others') + document.lower().count('researchers')\n",
    "            features['Number of occurrences of others or researchers'] = var\n",
    "        \n",
    "        # Count of numbers in the document\n",
    "        if control[17] == 1:\n",
    "            numbers = re.findall(r'\\b\\d+\\b', document)  # Using regex to find all numbers\n",
    "            count_numbers = len(numbers)\n",
    "            features['Count of numbers in the document'] = count_numbers\n",
    "\n",
    "        # Contains 2 times more capitals than periods\n",
    "        if control[18] == 1:\n",
    "            count_capitals = sum(1 for char in document if char.isupper())\n",
    "            count_periods = document.count('.')\n",
    "            \n",
    "            if count_periods > 0 and count_capitals >= 2 * count_periods:\n",
    "                features['Contains 2 times more capitals than periods'] = 1\n",
    "            else:\n",
    "                features['Contains 2 times more capitals than periods'] = 0\n",
    "\n",
    "        # Number of occurrences of 'et'\n",
    "        if control[19] == 1:\n",
    "            var = document.count('et')\n",
    "            features['Number of occurrences of et'] = var\n",
    "\n",
    "        # Sentence Mean\n",
    "        if control[20] == 1:\n",
    "            mean_length = np.mean(sentence_lengths)\n",
    "            features['Sentence Mean'] = mean_length\n",
    "        \n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stats\n",
      "====================\n",
      "Training Set Size: 33022\n",
      "X_train Size: (1501, 21)\n",
      "y_train Size: (1501,)\n",
      "Testing Set Size: 11000\n",
      "X_test Size: (500, 21)\n",
      "y_test Size: (500,)\n"
     ]
    }
   ],
   "source": [
    "# Get Feature set\n",
    "#case_BoolTF = 1\n",
    "#control = [1,1,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,1,1,0,1]\n",
    "\n",
    "if case_BoolTF == 1:\n",
    "    # Creat traing and testing split\n",
    "    FCount = int(len(set(all_fetr))/1)\n",
    "    word_features = list(all_fetr.keys())[:FCount]\n",
    "    featuresets = [({**find_features_stats(text, control), **find_features_boolean(text, word_features)}, category) for (text, category) in documents]\n",
    "    print(\"Using True/False\")\n",
    "elif case_BoolTF == 2:\n",
    "    # Creat traing and testing split\n",
    "    FCount = int(len(set(all_fetr))/1)\n",
    "    word_features = list(all_fetr.keys())[:FCount]\n",
    "    featuresets = [({**find_features_stats(text, control), **find_features_count(text, word_features)}, category) for (text, category) in documents]\n",
    "    print(\"Using Count\")\n",
    "elif case_BoolTF == 3:\n",
    "    featuresets = [(find_features_stats(text,control), category) for (text, category) in documents]\n",
    "    print(\"Using Stats\")\n",
    "else:\n",
    "    print(f\"Invalid case value: {case_BoolTF}\")\n",
    "random.shuffle(featuresets)\n",
    "if(TestingOutput == True):\n",
    "        print(featuresets[:2])\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#TestingOutput = True\n",
    "# Get testing/training data set\n",
    "Trainn = round(len(featuresets)*.75)\n",
    "training_set = featuresets[:Trainn]\n",
    "testing_set = featuresets[-(len(featuresets)-Trainn):]\n",
    "\n",
    "if(TestingOutput == True):\n",
    "    # Print the number of elements in each set\n",
    "    print(\"Number of elements in training_set:\", len(training_set))\n",
    "    print(\"Number of elements in testing_set:\", len(testing_set))\n",
    "    print(\"Check they add up: \",(len(testing_set)+len(training_set)))\n",
    "\n",
    "# Convert features to a numerical format using DictVectorizer\n",
    "vectorizer = DictVectorizer()\n",
    "X_train = vectorizer.fit_transform([features for (features, label) in training_set])\n",
    "y_train = np.array([label for (features, label) in training_set])\n",
    "\n",
    "X_test = vectorizer.transform([features for (features, label) in testing_set])\n",
    "y_test = np.array([label for (features, label) in testing_set])\n",
    "\n",
    "# Create an imputer\n",
    "imputer = SimpleImputer(strategy='mean')  # You can choose a different strategy based on your data\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Fit and transform the testing data\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "if(1 == 1):\n",
    "    print(\"====================\")\n",
    "    # Training set size\n",
    "    training_set_size = X_train.size + y_train.size\n",
    "    print(f\"Training Set Size: {training_set_size}\")\n",
    "    print(f\"X_train Size: {X_train.shape}\")\n",
    "    print(f\"y_train Size: {y_train.shape}\")\n",
    "\n",
    "    # Testing set size\n",
    "    testing_set_size = X_test.size + y_test.size\n",
    "    print(f\"Testing Set Size: {testing_set_size}\")\n",
    "    print(f\"X_test Size: {X_test.shape}\")\n",
    "    print(f\"y_test Size: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 752, number of negative: 749\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1579\n",
      "[LightGBM] [Info] Number of data points in the train set: 1501, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500999 -> initscore=0.003997\n",
      "[LightGBM] [Info] Start training from score 0.003997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, hidden_layer_sizes=(185,))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, hidden_layer_sizes=(185,))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.01, hidden_layer_sizes=(185,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Light Gradent Boosted Model\n",
    "lgbm_classifier = LGBMClassifier(max_depth=15,learning_rate=0.5,n_estimators=80)\n",
    "lgbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(max_depth=None, n_estimators=170)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# K Nearest Nabirs\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# MLPClassifier\n",
    "mlp_classifier = MLPClassifier( alpha=0.01, hidden_layer_sizes=(185,) )\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Light Gradient Boosted Model\n",
    "lgbm_class_probabilities = lgbm_classifier.predict_proba(X_test)\n",
    "lgbm_predictions = lgbm_classifier.predict(X_test)\n",
    "\n",
    "# RandomForestClassifier\n",
    "rf_class_probabilities = rf_classifier.predict_proba(X_test)\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# K Nearest Neighbors\n",
    "knn_class_probabilities = knn_classifier.predict_proba(X_test)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "# MLPClassifier\n",
    "mlp_class_probabilities = mlp_classifier.predict_proba(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 752, number of negative: 749\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1579\n",
      "[LightGBM] [Info] Number of data points in the train set: 1501, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500999 -> initscore=0.003997\n",
      "[LightGBM] [Info] Start training from score 0.003997\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Type 1 - Equal Voting Accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a VotingClassifier with equal weights for each model\n",
    "equal_voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm', lgbm_classifier),\n",
    "        ('rf', rf_classifier),\n",
    "        ('knn', knn_classifier),\n",
    "        ('mlp', mlp_classifier)\n",
    "    ],\n",
    "    voting='hard'  # Hard voting: each model gets one vote\n",
    ")\n",
    "\n",
    "# Fit the ensemble model on the training data\n",
    "equal_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the ensemble model\n",
    "equal_voting_predictions = equal_voting_classifier.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "equal_voting_accuracy = accuracy_score(y_test, equal_voting_predictions)\n",
    "print(f\"Type 1 - Equal Voting Accuracy: {equal_voting_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed Class Labels (first 5 rows): ['gen' 'gen' 'gen' 'org' 'org']\n",
      "Type 2 - Summed Probabilities Accuracy: 0.8320\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum the probabilities for each class\n",
    "summed_probabilities = (\n",
    "    lgbm_class_probabilities +\n",
    "    rf_class_probabilities +\n",
    "    knn_class_probabilities +\n",
    "    mlp_class_probabilities\n",
    ")\n",
    "\n",
    "# Choose the class with the higher probability for each sample\n",
    "summed_predictions = np.argmax(summed_probabilities, axis=1)\n",
    "\n",
    "# Convert the numerical predictions to class labels\n",
    "summed_class_labels = np.array(['gen', 'org'])[summed_predictions]\n",
    "\n",
    "# Print the first few rows of summed_class_labels for debugging\n",
    "print(\"Summed Class Labels (first 5 rows):\", summed_class_labels[:5])\n",
    "\n",
    "# Print accuracy\n",
    "summed_accuracy = accuracy_score(y_test, summed_class_labels)\n",
    "print(f\"Type 2 - Summed Probabilities Accuracy: {summed_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[[9.99963573e-01 8.35294118e-01 6.66666667e-01 7.18558410e-01]\n",
      " [2.70934169e-04 1.82352941e-01 5.55555556e-01 3.31947635e-01]\n",
      " [9.99989349e-01 9.64705882e-01 1.00000000e+00 9.95795594e-01]\n",
      " ...\n",
      " [6.52530026e-06 2.94117647e-02 0.00000000e+00 2.53706894e-02]\n",
      " [9.99940960e-01 9.11764706e-01 6.66666667e-01 8.84566021e-01]\n",
      " [1.00000000e+00 9.82352941e-01 1.00000000e+00 9.99998478e-01]]\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "MLP Combined Model Accuracy: 0.8380\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get probabilities on the training data\n",
    "lgbm_train_probs = lgbm_classifier.predict_proba(X_train)\n",
    "rf_train_probs = rf_classifier.predict_proba(X_train)\n",
    "knn_train_probs = knn_classifier.predict_proba(X_train)\n",
    "mlp_train_probs = mlp_classifier.predict_proba(X_train)\n",
    "\n",
    "# Combine probabilities into a single array for each sample\n",
    "combined_train_probs = np.column_stack((lgbm_train_probs[:, 1], rf_train_probs[:, 1], knn_train_probs[:, 1], mlp_train_probs[:, 1]))\n",
    "\n",
    "# Create a simple MLP with one layer and four nodes\n",
    "mlp_combined = MLPClassifier(hidden_layer_sizes=(20,), max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the MLP on the combined probabilities\n",
    "mlp_combined.fit(combined_train_probs, y_train)\n",
    "\n",
    "# Get probabilities on the test data\n",
    "lgbm_test_probs = lgbm_classifier.predict_proba(X_test)\n",
    "rf_test_probs = rf_classifier.predict_proba(X_test)\n",
    "knn_test_probs = knn_classifier.predict_proba(X_test)\n",
    "mlp_test_probs = mlp_classifier.predict_proba(X_test)\n",
    "\n",
    "# Combine probabilities into a single array for each sample\n",
    "combined_test_probs = np.column_stack((lgbm_test_probs[:, 1], rf_test_probs[:, 1], knn_test_probs[:, 1], mlp_test_probs[:, 1]))\n",
    "\n",
    "# Predict using the trained MLP on the combined probabilities of the test data\n",
    "mlp_combined_predictions = mlp_combined.predict(combined_test_probs)\n",
    "\n",
    "# Print accuracy\n",
    "mlp_combined_accuracy = accuracy_score(y_test, mlp_combined_predictions)\n",
    "print(f\"MLP Combined Model Accuracy: {mlp_combined_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Best Parameters: {'alpha': 0.0001, 'hidden_layer_sizes': (5,)}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "MLP Combined Model Accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get probabilities on the training data\n",
    "lgbm_train_probs = lgbm_classifier.predict_proba(X_train)\n",
    "rf_train_probs = rf_classifier.predict_proba(X_train)\n",
    "knn_train_probs = knn_classifier.predict_proba(X_train)\n",
    "mlp_train_probs = mlp_classifier.predict_proba(X_train)\n",
    "\n",
    "# Combine probabilities into a single array for each sample\n",
    "combined_train_probs = np.column_stack((lgbm_train_probs[:, 1], rf_train_probs[:, 1], knn_train_probs[:, 1], mlp_train_probs[:, 1]))\n",
    "\n",
    "# Impute missing values if any (replace this with your actual imputation strategy)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(combined_train_probs)\n",
    "\n",
    "# Define the parameter grid for architecture search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(1,), (5,), (10,), (20,), (30,), (1,5), (5,5), (10,5), (20,5), (30,5),(1,10), (5,10), (10,10), (20,10), (30,10)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Create MLPClassifier\n",
    "mlp_combined = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(mlp_combined, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Get the best parameters from the search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Use the best parameters to train the final model\n",
    "best_mlp_combined = MLPClassifier(hidden_layer_sizes=best_params['hidden_layer_sizes'], alpha=best_params['alpha'], max_iter=1000, random_state=42)\n",
    "best_mlp_combined.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Get probabilities on the test data\n",
    "lgbm_test_probs = lgbm_classifier.predict_proba(X_test)\n",
    "rf_test_probs = rf_classifier.predict_proba(X_test)\n",
    "knn_test_probs = knn_classifier.predict_proba(X_test)\n",
    "mlp_test_probs = mlp_classifier.predict_proba(X_test)\n",
    "\n",
    "# Combine probabilities into a single array for each sample\n",
    "combined_test_probs = np.column_stack((lgbm_test_probs[:, 1], rf_test_probs[:, 1], knn_test_probs[:, 1], mlp_test_probs[:, 1]))\n",
    "\n",
    "# Impute missing values if any (replace this with your actual imputation strategy)\n",
    "X_test_imputed = imputer.transform(combined_test_probs)\n",
    "\n",
    "# Predict using the trained MLP on the combined probabilities of the test data\n",
    "mlp_combined_predictions = best_mlp_combined.predict(X_test_imputed)\n",
    "\n",
    "# Print accuracy\n",
    "mlp_combined_accuracy = accuracy_score(y_test, mlp_combined_predictions)\n",
    "print(f\"MLP Combined Model Accuracy: {mlp_combined_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for the first bar\n",
    "bar1_data = [1,1,0,1]\n",
    "\n",
    "# Data for the second bar\n",
    "bar2_data = [1 - x for x in bar1_data]\n",
    "\n",
    "# Colors for each part of the bars\n",
    "colors = ['#4B0082', '#9370DB', '#778899', '#48D1CC']\n",
    "\n",
    "# Positions for each part of the bars\n",
    "positions = np.arange(len(bar1_data))\n",
    "\n",
    "# Create the bar chart\n",
    "plt.bar(0, bar1_data[0], color=colors[0], label='Bar 1')\n",
    "plt.bar(0, bar1_data[1], bottom=bar1_data[0], color=colors[1])\n",
    "plt.bar(0, bar1_data[2], bottom=bar1_data[0] + bar1_data[1], color=colors[2])\n",
    "plt.bar(0, bar1_data[3], bottom=bar1_data[0] + bar1_data[1] + bar1_data[2], color=colors[3])\n",
    "\n",
    "plt.bar(1, bar2_data[0], color=colors[0], label='Bar 2')\n",
    "plt.bar(1, bar2_data[1], bottom=bar2_data[0], color=colors[1])\n",
    "plt.bar(1, bar2_data[2], bottom=bar2_data[0] + bar2_data[1], color=colors[2])\n",
    "plt.bar(1, bar2_data[3], bottom=bar2_data[0] + bar2_data[1] + bar2_data[2], color=colors[3])\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('')\n",
    "plt.xticks([0, 1], ['Generated', 'Human'])\n",
    "\n",
    "# Remove y-axis labels\n",
    "plt.yticks([])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
